{"cells":[{"cell_type":"code","execution_count":51,"metadata":{"id":"CVUez91kL5kY","executionInfo":{"status":"ok","timestamp":1678816838537,"user_tz":-330,"elapsed":26265,"user":{"displayName":"Ravi Dhankani","userId":"03727704259725912723"}}},"outputs":[],"source":["import openai  # for OpenAI API calls\n","from openai.embeddings_utils import get_embedding\n","from tenacity import (\n","    retry,\n","    wait_none,\n","    stop_after_attempt,\n","    wait_random_exponential,\n",")\n","\n","#@retry(wait=wait_none(), stop=stop_after_attempt(6))\n","@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n","def handling_rate_error(**kwargs):\n","  df['embedding'] = new_df.apply(lambda x: get_embedding(x, engine='text-embedding-ada-002') )\n","  df.to_csv('word_embeddings_e_v.csv')\n","\n","handling_rate_error()"]},{"cell_type":"markdown","metadata":{"id":"golEYAY3lHVj"},"source":["# OpenAI Word Embeddings, Semantic Search\n","\n","Word embeddings are a way of representing words and phrases as vectors. They can be used for a variety of tasks, including semantic search, anomaly detection, and classification. In this tutorial, we will learn how to implement semantic search using OpenAI embeddings.\n","\n","To get started, we will need to install and import OpenAI and input an API Key. "]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"59HHMZJFtZLy","executionInfo":{"status":"ok","timestamp":1678885629849,"user_tz":-330,"elapsed":24329,"user":{"displayName":"Ravi Dhankani","userId":"03727704259725912723"}},"outputId":"845dcb3b-9409-43a1-bbb0-86e0311b435e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"AUMmdUS_LPkI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678886023410,"user_tz":-330,"elapsed":15999,"user":{"displayName":"Ravi Dhankani","userId":"03727704259725912723"}},"outputId":"8db7dd19-5826-4cd5-bc1e-71b22015bd77"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install --upgrade pip -q\n","!pip install openai  -q"]},{"cell_type":"code","source":["!pip install tiktoken -q"],"metadata":{"id":"yD7qy3e5osvw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678886028566,"user_tz":-330,"elapsed":5163,"user":{"displayName":"Ravi Dhankani","userId":"03727704259725912723"}},"outputId":"4ce6f32f-3cc5-475a-caa6-eddad61a0b26"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m1.3/1.7 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","execution_count":5,"metadata":{"id":"bUPM0-8iLNK0","executionInfo":{"status":"ok","timestamp":1678886029332,"user_tz":-330,"elapsed":774,"user":{"displayName":"Ravi Dhankani","userId":"03727704259725912723"}}},"outputs":[],"source":["import openai\n","import pandas as pd\n","import numpy as np\n","\n","# from getpass import getpass\n","#from openai.embeddings_utils import get_embedding\n","# from getpass import getpass\n","\n","\n","openai.api_key = 'sk-xxxxxxxxxxxx1NEEtRZ2aGGYsaZp'"]},{"cell_type":"markdown","metadata":{"id":"9KXdnqkoyK9H"},"source":["# Read Data File Containing Words\n","\n","Now that we have configured OpenAI, let's start with importing the data into google colab. Upload it to Google Colab. Once the file is uploaded, let's read it into a pandas dataframe using the code below:\n","!!write code to 'combine' and produce the column,"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"rHJ-2gvfx9-J","executionInfo":{"status":"ok","timestamp":1678886029333,"user_tz":-330,"elapsed":8,"user":{"displayName":"Ravi Dhankani","userId":"03727704259725912723"}}},"outputs":[],"source":["df = pd.read_csv('/content/drive/MyDrive/Trainchat five combine.csv', encoding_errors='ignore')\n","\n","df['Combined'] = 'Question: ' + df['prompt'] + '; ' + 'Answer: ' + df['Completion']\n","new_df = df['Combined'].dropna()"]},{"cell_type":"markdown","metadata":{"id":"XwUiwvTmL71c"},"source":["# Calculate Word Embeddings\n","\n","To use word embeddings for semantic search, you first compute the embeddings for a corpus of text using a word embedding algorithm. What does this mean? We are going to create a numerical representation of each of these words. To perform this computation, we'll use OpenAI's 'get_embedding' function. \n","\n","Since we have our words in a pandas dataframe, we can use \"apply\" to apply the get_embedding function to each row in the dataframe. We then store the calculated word embeddings in a new text file called \"word_embeddings.csv\" so that we don't have to call OpenAI again to perform these calculations."]},{"cell_type":"markdown","metadata":{"id":"e2WjqR_PzMtg"},"source":["# Semantic Search\n","\n","Now that we have our word embeddings stored, let's load them into a new dataframe and use it for semantic search. Since the 'embedding' in the CSV is stored as a string, we'll use apply() and to interpret this string as Python code and convert it to a numpy array so that we can perform calculations on it."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"yM6N30oYeWhs","executionInfo":{"status":"error","timestamp":1678886030023,"user_tz":-330,"elapsed":698,"user":{"displayName":"Ravi Dhankani","userId":"03727704259725912723"}},"colab":{"base_uri":"https://localhost:8080/","height":275},"outputId":"3e63b9ec-ed6c-4c70-f33d-e7cd7c04cf49"},"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-1cabf67edd0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdfe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/word_embeddings_e_v.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdfe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embedding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embedding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4431\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4432\u001b[0m         \"\"\"\n\u001b[0;32m-> 4433\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4435\u001b[0m     def _reduce(\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                 \u001b[0;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1144\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: eval() arg 1 must be a string, bytes or code object"]}],"source":["dfe = pd.read_csv('/content/drive/MyDrive/word_embeddings_e_v.csv')\n","dfe['embedding'] = dfe['embedding'].apply(eval).apply(np.array)\n","dfe.head(2)"]},{"cell_type":"code","source":["from math import sqrt \n","\n","def euclidean_distance(x,y):\n","  return sqrt(sum(pow(a-b,2) for a, b in zip(x, y)))"],"metadata":{"id":"KM_Rpq9edhXl","executionInfo":{"status":"aborted","timestamp":1678886030024,"user_tz":-330,"elapsed":15,"user":{"displayName":"Ravi Dhankani","userId":"03727704259725912723"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import openai\n","\n","def callGPT(prompt):\n","    response = openai.Completion.create(\n","    model=\"text-davinci-003\",\n","    prompt=prompt,\n","    temperature=0.1,\n","    max_tokens=500,\n","    frequency_penalty=0,\n","    presence_penalty=0\n","  )\n","    return response\n","\n"],"metadata":{"id":"ZrzkzHdSeKaU","executionInfo":{"status":"aborted","timestamp":1678886030025,"user_tz":-330,"elapsed":16,"user":{"displayName":"Ravi Dhankani","userId":"03727704259725912723"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oCtyD-yZz-6W"},"source":["Let's now prompt ourselves for a search term that isn't in the dataframe. We'll use word embeddings to perform a semantic search for the words that are most similar to the word we entered. I'll first try the word \"hot dog\". Then we'll come back and try the word \"yellow\"."]},{"cell_type":"markdown","metadata":{"id":"kpFRGaAX0H82"},"source":["Now that we have a search term, let's calculate an embedding or vector for that search term using the OpenAI get_embedding function."]},{"cell_type":"code","source":["def num_tokens_from_string(string: str, encoding_name: str) -> int:\n","    \"\"\"Returns the number of tokens in a text string.\"\"\"\n","    encoding = tiktoken.get_encoding(encoding_name)\n","    num_tokens = len(encoding.encode(string))\n","    return num_tokens\n","\n"],"metadata":{"id":"tffNe6xNtig4","executionInfo":{"status":"aborted","timestamp":1678886030028,"user_tz":-330,"elapsed":18,"user":{"displayName":"Ravi Dhankani","userId":"03727704259725912723"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WtyaOReqzzn3","executionInfo":{"status":"aborted","timestamp":1678886030028,"user_tz":-330,"elapsed":18,"user":{"displayName":"Ravi Dhankani","userId":"03727704259725912723"}}},"outputs":[],"source":["import tiktoken\n","from openai.embeddings_utils import cosine_similarity\n","\n","\n","# semantic search\n","def searchDB(search_term,df):\n","    search_term_vector = get_embedding(search_term, engine=\"text-embedding-ada-002\")\n","\n","    df[\"similarities\"] = df['embedding'].apply(lambda x: euclidean_distance(x, search_term_vector))\n","\n","    x = df.sort_values(\"similarities\", ascending=True).head(2)\n","    x\n","    combined = ''\n","\n","    data = x['Combined']\n","    data_list = data.values.tolist()\n","    print(data_list)\n","\n","    string = ''\n","\n","    for data in data_list:\n","      string+=data\n","      string += \"\\n\"\n","\n","    print(len(string))\n","\n","    MAX_SECTION_LEN = 500\n","    SEPARATOR = \"\\n* \"\n","    ENCODING = \"gpt2\"  # encoding for text-davinci-003\n","\n","    encoding = tiktoken.get_encoding(ENCODING)\n","    separator_len = len(encoding.encode(SEPARATOR))\n","\n","    num_tokens_from_string(string, \"gpt2\")"]},{"cell_type":"markdown","metadata":{"id":"KVSNY0Ci0hB5"},"source":[" Once we have a vector representing that word, we can see how similar it is to other words in our dataframe by calculating the cosine similarity of our search term's word vector to each word embedding in our dataframe."]},{"cell_type":"markdown","metadata":{"id":"VsqkYbCD05TC"},"source":["# Sorting By Similarity\n","\n","Now that we have calculated the similarities to each term in our dataframe, we simply sort the similarity values to find the terms that are most similar to the term we searched for. Notice how the foods are most similar to \"hot dog\". Not only that, it puts fast food closer to hot dog. Also some colors are ranked closer to hot dog than others. Let's go back and try the word \"yellow\" and walk through the results."]},{"cell_type":"code","source":["search_term = 'what is section 87A income tax'\n","searchDB(search_term,df)\n","prompt =f'Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don\\'t know.\\n\\nContext:\\n{string} \\nQuestion:{search_term}\\nAnswer:'\n","print(prompt)\n","response = callGPT(prompt)\n","response[\"choices\"][0][\"text\"].strip(\" \\n\")"],"metadata":{"id":"Ddoj3zMcc1Hc","executionInfo":{"status":"aborted","timestamp":1678886030029,"user_tz":-330,"elapsed":18,"user":{"displayName":"Ravi Dhankani","userId":"03727704259725912723"}}},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1RLWEs4RaAKV-C7Xe3gGhCzURfPIyaJsd","timestamp":1676526414931}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
